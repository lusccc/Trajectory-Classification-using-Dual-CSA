{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tables as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from MF_RP_mat_h5support import H5_NODE_NAME\n",
    "m = nn.Conv2d(16, 33, 3, padding=0)\n",
    "\n",
    "input = torch.randn(20,16,50,100)\n",
    "_input = input.numpy()\n",
    "out = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "input = torch.tensor([[[[ 1.,  2,  3,  4],\n",
    "                            [ 5,  6,  7,  8],\n",
    "                            [ 9, 10, 11, 12],\n",
    "                            [13, 14, 15, 16]]]])\n",
    "output, indices = pool(input)\n",
    "a=pool(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn((3, 10))\n",
    "b = torch.randn((3,10))\n",
    "c = a -b\n",
    "d= c**2\n",
    "dT= d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids(u):\n",
      "[[1 1 1 1]\n",
      " [3 3 3 3]\n",
      " [6 6 6 6]]\n",
      "---\n",
      "embs(z):\n",
      "[[[13 14 15 16]]\n",
      "\n",
      " [[17 18 19 20]]]\n",
      "---\n",
      "z_minus_u:\n",
      "[[[12 13 14 15]\n",
      "  [10 11 12 13]\n",
      "  [ 7  8  9 10]]\n",
      "\n",
      " [[16 17 18 19]\n",
      "  [14 15 16 17]\n",
      "  [11 12 13 14]]]\n",
      "---\n",
      "z_minus_u_square:\n",
      "[[[144 169 196 225]\n",
      "  [100 121 144 169]\n",
      "  [ 49  64  81 100]]\n",
      "\n",
      " [[256 289 324 361]\n",
      "  [196 225 256 289]\n",
      "  [121 144 169 196]]]\n",
      "---\n",
      "dist_square_sum:\n",
      "[[ 734  534  294]\n",
      " [1230  966  630]]\n",
      "---\n",
      "qij:\n",
      "[[0.00136054 0.00186916 0.00338983]\n",
      " [0.00081235 0.00103413 0.00158479]]\n",
      "---\n",
      "qij_sum:\n",
      "[0.00661953 0.00343126]\n",
      "---\n",
      "qij_t:\n",
      "[[0.00136054 0.00081235]\n",
      " [0.00186916 0.00103413]\n",
      " [0.00338983 0.00158479]]\n",
      "---\n",
      "qij_normalize:\n",
      "[[0.20553476 0.2367491 ]\n",
      " [0.28237018 0.3013838 ]\n",
      " [0.51209507 0.4618671 ]]\n",
      "---\n",
      "qij_normalize_t:\n",
      "[[0.20553476 0.28237018 0.51209507]\n",
      " [0.2367491  0.3013838  0.4618671 ]]\n"
     ]
    }
   ],
   "source": [
    "#***STUDENT T DISTRIBUTION TEST***\n",
    "#q_ij = 1/(1+dist(z_i, u_j)^2), then normalize it.\n",
    "centroids = np.array(\n",
    "    [[1,1,1,1],\n",
    "    [3,3,3,3],\n",
    "    [6,6,6,6]]\n",
    ") #u, 3 classes, each class centroid is 4D vector\n",
    "emb1 = np.array([[13,14,15,16],])\n",
    "emb2 = np.array([[17,18,19,20],])\n",
    "embs = np.array([emb1, emb2])#z, 2 samples, each sample embedded to 4D vector\n",
    "\n",
    "z_minus_u = embs - centroids # diff between each z and each centroids\n",
    "print('centroids(u):')\n",
    "print(centroids)\n",
    "print('---')\n",
    "print('embs(z):')\n",
    "print(embs)\n",
    "print('---')\n",
    "print('z_minus_u:')\n",
    "print(z_minus_u)\n",
    "z_minus_u_square = np.square(z_minus_u)  # i.e., distance square between each z and each centroids\n",
    "dist_square_sum = np.sum(z_minus_u_square, axis=2) # each element: square of euclid distance\n",
    "print('---')\n",
    "print('z_minus_u_square:')\n",
    "print(z_minus_u_square)\n",
    "print('---')\n",
    "print('dist_square_sum:')\n",
    "print(dist_square_sum)\n",
    "alpha = 1.\n",
    "qij = 1. / (1. + dist_square_sum / 1.) # each element: the probability of sample i belong to j\n",
    "print('---')\n",
    "print('qij:')\n",
    "print(qij)\n",
    "qij_sum = np.sum(qij, axis=1) # sum probability to all 3 centroid\n",
    "print('---')\n",
    "print('qij_sum:')\n",
    "print(qij_sum)\n",
    "qij_t = np.transpose(qij)\n",
    "print('---')\n",
    "print('qij_t:')\n",
    "print(qij_t)\n",
    "qij_normalize = qij_t / qij_sum # each probability/sum probability\n",
    "print('---')\n",
    "print('qij_normalize:')\n",
    "print(qij_normalize)\n",
    "print('---')\n",
    "print('qij_normalize_t:') # note !!qij_normalize_t each row sum to 1, which is suitable for kl_div loss\n",
    "print(qij_normalize.T) # transpose back to denote the probability of i belong to j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#How to calculate the mean and std of my own dataset?\n",
    "#https://discuss.pytorch.org/t/about-normalization-using-pre-trained-vgg16-networks/23560/23\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(1000, 3, 224, 224)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cpu\")\n",
    "    dataset = MyDataset()\n",
    "\n",
    "    start = timeit.time.perf_counter()\n",
    "    data = dataset.data.to(device)\n",
    "    print(\"Mean:\", torch.mean(data, dim=(0, 2, 3))) # get mean on each channel for all samples\n",
    "    print(\"Std:\", torch.std(data, dim=(0, 2, 3)))\n",
    "    print(\"Elapsed time: %.3f seconds\" % (timeit.time.perf_counter() - start))\n",
    "    print()\n",
    "\n",
    "    start = timeit.time.perf_counter()\n",
    "    mean = 0.\n",
    "    for data in dataset:\n",
    "        data = data.to(device)\n",
    "        mean += torch.mean(data, dim=(1, 2))\n",
    "    mean /= len(dataset)\n",
    "    print(\"Mean:\", mean)\n",
    "\n",
    "    temp = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in dataset:\n",
    "        data = data.to(device)\n",
    "        temp += ((data.view(3, -1) - mean.unsqueeze(1)) ** 2).sum(dim=1)\n",
    "        nb_samples += np.prod(data.size()[1:])\n",
    "    std = torch.sqrt(temp/nb_samples)\n",
    "    print(\"Std:\", std)\n",
    "    print(\"Elapsed time: %.3f seconds\" % (timeit.time.perf_counter() - start))\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_type = 'train'\n",
    "# RP_mats_h5array = tb.open_file(f'./data/SHL_features/RP_mats_{data_type}.h5', mode='r').get_node('/' + H5_NODE_NAME)\n",
    "# RP_mats_h5array.shape\n",
    "# e=RP_mats_h5array[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[12., 13., 14., 15.],\n",
      "         [16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.]]])\n",
      "tensor([10., 11., 12., 13.])\n",
      "tensor([[ 6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13.],\n",
      "        [14., 15., 16., 17.]])\n",
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(24, dtype=torch.float).view(2,3,4)\n",
    "print(x)\n",
    "x_mean0=torch.mean(x,dim=(0,1),keepdim=False)\n",
    "x_mean1=torch.mean(x,dim=0,keepdim=False)\n",
    "x_mean2=torch.mean(x,dim=1,keepdim=False)\n",
    "print(x_mean0)\n",
    "print(x_mean1)\n",
    "print(x_mean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lsc\\anaconda3\\envs\\p37\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss(size_average=True)\n",
    "input = Variable(torch.ones(2,2), requires_grad=True)\n",
    "target = Variable(torch.Tensor([[3,3],[3,3]]))\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "gra=input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn((3, 10))\n",
    "b = torch.randn((3,10))\n",
    "c = a -b\n",
    "d= c**2\n",
    "dT= d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids(u):\n",
      "[[[1 1 1 1]\n",
      "  [3 3 3 3]\n",
      "  [6 6 6 6]]\n",
      "\n",
      " [[1 1 1 1]\n",
      "  [3 3 3 3]\n",
      "  [6 6 6 6]]]\n",
      "---\n",
      "embs(z):\n",
      "[[[13 14 15 16]]\n",
      "\n",
      " [[17 18 19 20]]]\n",
      "---\n",
      "z_minus_u:\n",
      "[[[12 13 14 15]\n",
      "  [10 11 12 13]\n",
      "  [ 7  8  9 10]]\n",
      "\n",
      " [[16 17 18 19]\n",
      "  [14 15 16 17]\n",
      "  [11 12 13 14]]]\n",
      "---\n",
      "z_minus_u_square:\n",
      "[[[144 169 196 225]\n",
      "  [100 121 144 169]\n",
      "  [ 49  64  81 100]]\n",
      "\n",
      " [[256 289 324 361]\n",
      "  [196 225 256 289]\n",
      "  [121 144 169 196]]]\n",
      "---\n",
      "dist_square_sum:\n",
      "[[ 734  534  294]\n",
      " [1230  966  630]]\n",
      "---\n",
      "qij:\n",
      "[[0.00136054 0.00186916 0.00338983]\n",
      " [0.00081235 0.00103413 0.00158479]]\n",
      "---\n",
      "qij_sum:\n",
      "[0.00661953 0.00343126]\n",
      "---\n",
      "qij_t:\n",
      "[[0.00136054 0.00081235]\n",
      " [0.00186916 0.00103413]\n",
      " [0.00338983 0.00158479]]\n",
      "---\n",
      "qij_normalize:\n",
      "[[0.20553476 0.2367491 ]\n",
      " [0.28237018 0.3013838 ]\n",
      " [0.51209507 0.4618671 ]]\n",
      "---\n",
      "qij_normalize_t:\n",
      "[[0.20553476 0.28237018 0.51209507]\n",
      " [0.2367491  0.3013838  0.4618671 ]]\n"
     ]
    }
   ],
   "source": [
    "#***STUDENT T DISTRIBUTION TEST 2***\n",
    "#q_ij = 1/(1+dist(z_i, u_j)^2), then normalize it.\n",
    "centroids = np.array(\n",
    "    [[1,1,1,1],\n",
    "    [3,3,3,3],\n",
    "    [6,6,6,6]]\n",
    ") #u, 3 classes, each class centroid is 4D vector\n",
    "centroids = np.array([centroids, centroids]) # !note: repeated 2 times\n",
    "emb1 = np.array([[13,14,15,16],])\n",
    "emb2 = np.array([[17,18,19,20],])\n",
    "embs = np.array([emb1, emb2])#z, 2 samples, each sample embedded to 4D vector\n",
    "\n",
    "z_minus_u = embs - centroids # diff between each z and each centroids\n",
    "print('centroids(u):')\n",
    "print(centroids)\n",
    "print('---')\n",
    "print('embs(z):')\n",
    "print(embs)\n",
    "print('---')\n",
    "print('z_minus_u:')\n",
    "print(z_minus_u)\n",
    "z_minus_u_square = np.square(z_minus_u)  # i.e., distance square between each z and each centroids\n",
    "dist_square_sum = np.sum(z_minus_u_square, axis=2) # each element: square of euclid distance\n",
    "print('---')\n",
    "print('z_minus_u_square:')\n",
    "print(z_minus_u_square)\n",
    "print('---')\n",
    "print('dist_square_sum:')\n",
    "print(dist_square_sum)\n",
    "alpha = 1.\n",
    "qij = 1. / (1. + dist_square_sum / 1.) # each element: the probability of sample i belong to j\n",
    "print('---')\n",
    "print('qij:')\n",
    "print(qij)\n",
    "qij_sum = np.sum(qij, axis=1) # sum probability to all 3 centroid\n",
    "print('---')\n",
    "print('qij_sum:')\n",
    "print(qij_sum)\n",
    "qij_t = np.transpose(qij)\n",
    "print('---')\n",
    "print('qij_t:')\n",
    "print(qij_t)\n",
    "qij_normalize = qij_t / qij_sum # each probability/sum probability\n",
    "print('---')\n",
    "print('qij_normalize:')\n",
    "print(qij_normalize)\n",
    "print('---')\n",
    "print('qij_normalize_t:')\n",
    "print(qij_normalize.T) # transpose back to denote the probability of i belong to j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0863)\n",
      "0.08630010165195404\n",
      "tensor(0.0863)\n"
     ]
    }
   ],
   "source": [
    "# kld test, this is the same example in wiki\n",
    "P = torch.Tensor([0.36, 0.48, 0.16]) #traget\n",
    "Q = torch.Tensor([0.333, 0.333, 0.333]) #pred\n",
    "print(\n",
    "(P * (P / Q).log()).sum())\n",
    "# tensor(0.0863), 10.2 µs ± 508\n",
    "a = (0.36 * np.log(.36/.333) + 0.48 * np.log(.48/.333) + 0.16 * np.log(.16/.333))\n",
    "print(a)\n",
    "print(F.kl_div(Q.log(), P, None, None, 'sum'))\n",
    "# tensor(0.0863), 14.1 µs ± 408 ns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1285, -1.7640, -1.6139, -4.0444, -0.5653],\n",
      "        [-2.5421, -2.2013, -1.4105, -0.7695, -2.2693]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lsc\\anaconda3\\envs\\p37\\lib\\site-packages\\torch\\nn\\functional.py:2352: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor(0.1329), tensor(0.6643), tensor(0.1329))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kld test\n",
    "# https://github.com/pytorch/pytorch/pull/14457/files\n",
    "# https://github.com/pytorch/pytorch/issues/6622\n",
    "input_shape = (2, 5)\n",
    "log_prob1 = F.log_softmax(torch.randn(input_shape), 1)\n",
    "print(log_prob1)\n",
    "prob2 = F.softmax(torch.randn(input_shape), 1)\n",
    "loss = nn.KLDivLoss(reduction='mean') # wrong\n",
    "# loss = nn.KLDivLoss(reduction='batchmean') # right\n",
    "l = loss(log_prob1, prob2)\n",
    "loss_none_reduce = nn.KLDivLoss(reduction='sum')(log_prob1, prob2)\n",
    "expected = loss_none_reduce / input_shape[0]\n",
    "wrong_l = loss_none_reduce / (2*5) # element-wise indeed, which is wrong for kld loss\n",
    "l, expected, wrong_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(73., grad_fn=<AddBackward0>) tensor(2.) tensor(54.) None\n"
     ]
    }
   ],
   "source": [
    "# loss function weight test\n",
    "x1 = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = torch.tensor(2.0, requires_grad=True)\n",
    "z1 = x1**2+y1\n",
    "\n",
    "x2 = torch.tensor(3.0, requires_grad=True)\n",
    "y2 = torch.tensor(4.0, requires_grad=True)\n",
    "z2 = x2**3+2*y2\n",
    "\n",
    "x3 = torch.tensor(5.0, requires_grad=True)\n",
    "y3 = torch.tensor(6.0, requires_grad=True)\n",
    "z3 = x3**4+3*y3\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "gamma = 1\n",
    "\n",
    "Z = alpha*z1 + beta*z2 + gamma*z3\n",
    "Z.backward()\n",
    "print(Z, x1.grad, x2.grad, x3.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}